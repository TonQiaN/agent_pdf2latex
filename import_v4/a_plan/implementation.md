# V3 File-Based Workflow - å®ç°ç»†èŠ‚

## ç›®å½•ç»“æ„

```
import_v4/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ main.py                          # ä¸»å…¥å£ï¼ˆä»…è°ƒç”¨workflowï¼‰
â”œâ”€â”€ workflow.py                      # ğŸ†• å®Œæ•´å·¥ä½œæµé€»è¾‘
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.py                  # é…ç½®ç®¡ç†
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ schemas.py                   # Pydanticæ•°æ®æ¨¡å‹
â”œâ”€â”€ preprocessing/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ pdf_renderer.py              # PDFé¢„å¤„ç†å’Œæ¸²æŸ“
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ file_uploader.py             # ğŸ†• OpenAIæ–‡ä»¶ä¸Šä¼ æœåŠ¡
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ classifier_agent.py          # è¯•å·ç±»å‹åˆ†ç±»å™¨
â”‚   â”œâ”€â”€ question_lister_agent.py     # ğŸ†• é¢˜ç›®æ¸…å•Agent
â”‚   â”œâ”€â”€ file_based_question_processor.py  # ğŸ†• åŸºäºFile IDçš„é¢˜ç›®å¤„ç†Agent
â”‚   â”œâ”€â”€ prompts.py                   # Agentæç¤ºè¯
â”‚   â”œâ”€â”€ file_lister_prompts.py       # ğŸ†• Listerä¸“ç”¨æç¤ºè¯
â”‚   â”œâ”€â”€ file_processor_prompts.py    # ğŸ†• File Processorä¸“ç”¨æç¤ºè¯
â”‚   â””â”€â”€ safety_controller.py         # å®‰å…¨æ§åˆ¶å™¨
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ latex_compiler.py            # LaTeXç¼–è¯‘å·¥å…·
â”œâ”€â”€ postprocessing/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ image_extractor.py           # å›¾ç‰‡æå–
â”‚   â””â”€â”€ metadata_extractor.py        # å…ƒæ•°æ®æå–
â””â”€â”€ utils/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ logger.py                    # æ—¥å¿—é…ç½®
```

---

## è¯¦ç»†å®ç°

### 1. é…ç½®ç®¡ç† (`config/settings.py`)

```python
"""Configuration settings for import_v4"""

from pydantic_settings import BaseSettings
from typing import Optional


class Settings(BaseSettings):
    """Application settings"""
    
    # OpenAIé…ç½®
    openai_api_key: str
    openai_model: str = "gpt-4o"  # é»˜è®¤æ¨¡å‹
    
    # Agenté…ç½®
    max_turns_per_question: int = 15
    max_latex_fix_attempts: int = 2
    
    # æ–‡ä»¶ä¸Šä¼ é…ç½®
    file_upload_purpose: str = "assistants"
    auto_cleanup_files: bool = False  # æ˜¯å¦è‡ªåŠ¨æ¸…ç†ä¸Šä¼ çš„æ–‡ä»¶
    
    # åˆ†ç±»å™¨é…ç½®
    classifier_max_turns: int = 5
    classification_sample_pages: int = 3  # ç”¨äºåˆ†ç±»çš„é¡µé¢æ•°é‡ï¼ˆå–å€’æ•°ç¬¬2ã€4ã€6é¡µï¼Œæˆ–æœ€å3é¡µï¼‰
    
    # Listeré…ç½®
    lister_max_turns: int = 10
    
    # è¾“å‡ºé…ç½®
    output_dir: str = "output"
    save_question_list: bool = True  # æ˜¯å¦ä¿å­˜é¢˜ç›®æ¸…å•
    
    class Config:
        env_file = ".env"
        env_prefix = "EXAM_PROCESSOR_"


settings = Settings()
```

---

### 2. æ•°æ®æ¨¡å‹ (`models/schemas.py`)

```python
"""Pydantic data models for V3 workflow"""

from typing import List, Optional, Literal
from pydantic import BaseModel, Field, ConfigDict


# ============ åŸºç¡€æ¨¡å‹ ============

class ImageInfo(BaseModel):
    """å›¾ç‰‡ä¿¡æ¯"""
    model_config = ConfigDict(extra="forbid")
    
    page_number: int = Field(..., description="Page number where image appears")
    bbox: List[float] = Field(..., description="Bounding box [x1, y1, x2, y2]")
    description: Optional[str] = Field(None, description="Image description")
    image_path: Optional[str] = Field(None, description="Extracted image path")


# ============ åˆ†ç±»å™¨è¾“å‡º ============

class ExamTypeOutput(BaseModel):
    """è¯•å·ç±»å‹åˆ†ç±»è¾“å‡º"""
    model_config = ConfigDict(extra="forbid")
    
    exam_type: Literal["type1", "type2"] = Field(
        ...,
        description="type1: separate answer booklet, type2: answer on paper"
    )
    reasoning: str = Field(..., description="Classification reasoning")
    confidence: Optional[float] = Field(None, description="Confidence score 0-1")


# ============ Question Listerè¾“å‡º ============

class QuestionItem(BaseModel):
    """å•ä¸ªé¢˜ç›®ä¿¡æ¯ï¼ˆæ¥è‡ªListerï¼‰"""
    model_config = ConfigDict(extra="forbid")
    
    question_index: int = Field(..., description="Sequential index (1-based)")
    question_label: str = Field(
        ...,
        description="Question label as it appears in paper (e.g., '10(a)', 'Question 21')"
    )


class QuestionList(BaseModel):
    """é¢˜ç›®æ¸…å•ï¼ˆListerçš„è¾“å‡ºï¼‰"""
    model_config = ConfigDict(extra="forbid")
    
    exam_type: str = Field(..., description="type1 or type2")
    total_questions: int = Field(..., description="Total number of questions")
    questions: List[QuestionItem] = Field(..., description="List of all questions")
    
    def validate_consistency(self) -> bool:
        """éªŒè¯æ¸…å•ä¸€è‡´æ€§"""
        return len(self.questions) == self.total_questions


# ============ Question Processorè¾“å‡º ============

class QuestionOutput(BaseModel):
    """å•é“é¢˜ç›®çš„å¤„ç†è¾“å‡º"""
    model_config = ConfigDict(extra="forbid")
    
    question_index: int = Field(..., description="Sequential index")
    question_number: str = Field(..., description="Question label like '10(a)'")
    
    question_latex: str = Field(..., description="Question LaTeX code")
    answer_latex: str = Field(..., description="Answer LaTeX code")
    
    question_images: List[ImageInfo] = Field(
        default_factory=list,
        description="Images in question"
    )
    answer_images: List[ImageInfo] = Field(
        default_factory=list,
        description="Images in answer"
    )
    
    marks: Optional[int] = Field(None, description="Question marks")
    reasoning: Optional[str] = Field(None, description="Processing reasoning")


# ============ æœ€ç»ˆè¾“å‡º ============

class ProcessedExam(BaseModel):
    """å®Œæ•´è¯•å·å¤„ç†ç»“æœ"""
    model_config = ConfigDict(extra="forbid")
    
    exam_id: str = Field(..., description="Exam ID")
    exam_type: str = Field(..., description="type1 or type2")
    
    total_questions: int = Field(..., description="Total questions")
    questions: List[QuestionOutput] = Field(..., description="All processed questions")
    
    # æ–‡ä»¶ä¿¡æ¯
    paper_pdf_path: str = Field(..., description="Original paper PDF path")
    solution_pdf_path: str = Field(..., description="Original solution PDF path")
    paper_file_id: Optional[str] = Field(None, description="OpenAI paper file ID")
    solution_file_id: Optional[str] = Field(None, description="OpenAI solution file ID")
    
    # å…ƒæ•°æ®
    processing_time_seconds: Optional[float] = Field(None, description="Total processing time")
    workflow_version: str = Field(default="v3_file_based", description="Workflow version")
```

---

### 3. æ–‡ä»¶ä¸Šä¼ æœåŠ¡ (`services/file_uploader.py`) ğŸ†•

```python
"""File uploader service for OpenAI"""

from pathlib import Path
from typing import Dict, Optional
from openai import AsyncOpenAI
from loguru import logger

from ..config.settings import settings


class FileUploadResult:
    """æ–‡ä»¶ä¸Šä¼ ç»“æœ"""
    def __init__(
        self,
        paper_file_id: str,
        solution_file_id: str,
        paper_file,
        solution_file
    ):
        self.paper_file_id = paper_file_id
        self.solution_file_id = solution_file_id
        self.paper_file = paper_file
        self.solution_file = solution_file
    
    def to_dict(self) -> Dict[str, str]:
        return {
            "paper_file_id": self.paper_file_id,
            "solution_file_id": self.solution_file_id
        }


async def upload_pdfs_get_file_ids(
    paper_pdf_path: str,
    solution_pdf_path: str,
    client: Optional[AsyncOpenAI] = None
) -> FileUploadResult:
    """
    ä¸Šä¼ PDFåˆ°OpenAIï¼Œè·å–æŒä¹…åŒ–file_id
    
    Args:
        paper_pdf_path: Paper PDFè·¯å¾„
        solution_pdf_path: Solution PDFè·¯å¾„
        client: å¯é€‰çš„OpenAIå®¢æˆ·ç«¯ï¼ˆç”¨äºå¤ç”¨è¿æ¥ï¼‰
    
    Returns:
        FileUploadResult: åŒ…å«file_idçš„ç»“æœå¯¹è±¡
    
    Raises:
        FileNotFoundError: PDFæ–‡ä»¶ä¸å­˜åœ¨
        Exception: ä¸Šä¼ å¤±è´¥
    """
    # éªŒè¯æ–‡ä»¶å­˜åœ¨
    paper_path = Path(paper_pdf_path)
    solution_path = Path(solution_pdf_path)
    
    if not paper_path.exists():
        raise FileNotFoundError(f"Paper PDF not found: {paper_pdf_path}")
    if not solution_path.exists():
        raise FileNotFoundError(f"Solution PDF not found: {solution_pdf_path}")
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    if client is None:
        client = AsyncOpenAI(api_key=settings.openai_api_key)
    
    logger.info("ğŸ“¤ Uploading paper PDF to OpenAI...")
    logger.info(f"   File: {paper_path.name} ({paper_path.stat().st_size / 1024:.1f} KB)")
    
    try:
        with open(paper_pdf_path, 'rb') as f:
            paper_file = await client.files.create(
                file=f,
                purpose=settings.file_upload_purpose
            )
        logger.info(f"âœ“ Paper uploaded: {paper_file.id}")
    except Exception as e:
        logger.error(f"Failed to upload paper PDF: {e}")
        raise
    
    logger.info("ğŸ“¤ Uploading solution PDF to OpenAI...")
    logger.info(f"   File: {solution_path.name} ({solution_path.stat().st_size / 1024:.1f} KB)")
    
    try:
        with open(solution_pdf_path, 'rb') as f:
            solution_file = await client.files.create(
                file=f,
                purpose=settings.file_upload_purpose
            )
        logger.info(f"âœ“ Solution uploaded: {solution_file.id}")
    except Exception as e:
        logger.error(f"Failed to upload solution PDF: {e}")
        # æ¸…ç†å·²ä¸Šä¼ çš„paperæ–‡ä»¶
        try:
            await client.files.delete(paper_file.id)
            logger.info(f"âœ“ Cleaned up paper file: {paper_file.id}")
        except:
            pass
        raise
    
    return FileUploadResult(
        paper_file_id=paper_file.id,
        solution_file_id=solution_file.id,
        paper_file=paper_file,
        solution_file=solution_file
    )


async def cleanup_files(
    file_ids: list[str],
    client: Optional[AsyncOpenAI] = None
):
    """
    æ¸…ç†ä¸Šä¼ çš„æ–‡ä»¶
    
    Args:
        file_ids: æ–‡ä»¶IDåˆ—è¡¨
        client: å¯é€‰çš„OpenAIå®¢æˆ·ç«¯
    """
    if client is None:
        client = AsyncOpenAI(api_key=settings.openai_api_key)
    
    logger.info(f"ğŸ§¹ Cleaning up {len(file_ids)} files...")
    
    for file_id in file_ids:
        try:
            await client.files.delete(file_id)
            logger.info(f"âœ“ File deleted: {file_id}")
        except Exception as e:
            logger.warning(f"Failed to delete file {file_id}: {e}")


async def verify_file_exists(
    file_id: str,
    client: Optional[AsyncOpenAI] = None
) -> bool:
    """
    éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨äºOpenAI
    
    Args:
        file_id: æ–‡ä»¶ID
        client: å¯é€‰çš„OpenAIå®¢æˆ·ç«¯
    
    Returns:
        True if file exists, False otherwise
    """
    if client is None:
        client = AsyncOpenAI(api_key=settings.openai_api_key)
    
    try:
        file_info = await client.files.retrieve(file_id)
        logger.info(f"âœ“ File exists: {file_id} ({file_info.filename})")
        return True
    except Exception as e:
        logger.warning(f"File not found: {file_id} ({e})")
        return False
```

---

### 4. è½»é‡çº§é¢„å¤„ç† (`preprocessing/pdf_renderer.py`)

åœ¨ç°æœ‰ä»£ç åŸºç¡€ä¸Šæ·»åŠ æ–°å‡½æ•°ï¼š

```python
"""PDF rendering and preprocessing"""

import fitz  # PyMuPDF
from typing import Dict, List
from loguru import logger


async def preprocess_for_classification(paper_pdf_path: str) -> Dict:
    """
    è½»é‡çº§é¢„å¤„ç†ï¼šæ¸²æŸ“æŒ‡å®šé¡µé¢ç”¨äºåˆ†ç±»
    
    ç­–ç•¥ï¼ˆä¸ import_v3 ä¸€è‡´ï¼‰ï¼š
    - ä¼˜å…ˆä½¿ç”¨å€’æ•°ç¬¬ 2ã€4ã€6 é¡µ
    - å¦‚æœé¡µæ•°ä¸è¶³ï¼Œä½¿ç”¨æœ€å N é¡µ
    
    Args:
        paper_pdf_path: Paper PDFè·¯å¾„
    
    Returns:
        {
            "selected_pages": [page1_data, page2_data, ...],
            "paper_pdf_path": str,
            "total_pages": int
        }
    """
    from ..config.settings import settings
    import base64
    
    logger.info(f"ğŸ“„ Preprocessing for classification...")
    
    doc = fitz.open(paper_pdf_path)
    total_pages = len(doc)
    
    logger.info(f"   Total pages: {total_pages}")
    
    # é€‰æ‹©é¡µé¢ï¼šä¼˜å…ˆä½¿ç”¨å€’æ•°ç¬¬ 2ã€4ã€6 é¡µ
    target_indices = []
    for offset in [2, 4, 6]:
        idx = total_pages - offset
        if idx >= 0:
            target_indices.append(idx)
    
    # å¦‚æœé¡µæ•°ä¸è¶³ï¼Œä½¿ç”¨æœ€å N é¡µ
    if len(target_indices) < settings.classification_sample_pages:
        target_indices = list(range(
            max(0, total_pages - settings.classification_sample_pages), 
            total_pages
        ))
    
    # æ’åºä»¥ä¿æŒé¡µé¢é¡ºåº
    target_indices.sort()
    
    # é¡µç ï¼ˆ1-basedï¼‰
    page_numbers = [idx + 1 for idx in target_indices]
    logger.info(f"   Selected pages for classification: {page_numbers}")
    
    # æ¸²æŸ“é€‰ä¸­çš„é¡µé¢
    selected_pages = []
    for idx in target_indices:
        page_num = idx + 1
        logger.info(f"   Rendering page {page_num}...")
        page = doc[idx]
        
        # æ¸²æŸ“ä¸ºå›¾ç‰‡
        pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # 2x scale
        img_bytes = pix.tobytes("png")
        
        # Base64ç¼–ç 
        img_base64 = base64.b64encode(img_bytes).decode('utf-8')
        
        page_data = {
            "page_number": page_num,
            "image_base64": img_base64,
            "width": pix.width,
            "height": pix.height
        }
        selected_pages.append(page_data)
    
    doc.close()
    
    logger.info(f"âœ“ Preprocessed {len(selected_pages)} pages for classification")
    
    return {
        "selected_pages": selected_pages,
        "paper_pdf_path": paper_pdf_path,
        "total_pages": total_pages
    }


# ç°æœ‰çš„preprocess_pdfså‡½æ•°ä¿æŒä¸å˜ï¼ˆç”¨äºå…¶ä»–æ¨¡å¼ï¼‰
```

---

### 5. è¯•å·ç±»å‹åˆ†ç±»å™¨ (`agents/classifier_agent.py`)

```python
"""Exam Type Classifier Agent"""

from loguru import logger
from agents import Agent, Runner

from ..config.settings import settings
from ..models.schemas import ExamTypeOutput


async def classify_exam_type(classification_data: dict) -> str:
    """
    ä½¿ç”¨é€‰å®šçš„é¡µé¢è¿›è¡Œè¯•å·ç±»å‹åˆ†ç±»
    
    ç­–ç•¥ï¼ˆä¸ import_v3 ä¸€è‡´ï¼‰ï¼š
    - ä½¿ç”¨å€’æ•°ç¬¬ 2ã€4ã€6 é¡µï¼ˆæˆ–æœ€å N é¡µï¼‰
    - è¿™äº›é¡µé¢é€šå¸¸åŒ…å«ç­”é¢˜åŒºåŸŸï¼Œæ›´å®¹æ˜“åˆ¤æ–­è¯•å·ç±»å‹
    
    Args:
        classification_data: é¢„å¤„ç†æ•°æ®ï¼ŒåŒ…å« selected_pages
    
    Returns:
        è¯•å·ç±»å‹å­—ç¬¦ä¸²: "type1" or "type2"
    """
    from .prompts import get_classifier_prompt
    
    classifier_agent = Agent(
        name="Exam Classifier",
        instructions=get_classifier_prompt(),
        output_type=ExamTypeOutput,
        model=settings.openai_model
    )
    
    selected_pages = classification_data["selected_pages"]
    page_numbers = [p["page_number"] for p in selected_pages]
    
    logger.info(f"ğŸ“Š Classifying exam type using pages: {page_numbers}")
    
    # æ„å»ºè¾“å…¥ï¼ŒåŒ…å«å®é™…çš„ base64 å›¾ç‰‡
    input_text = "Analyze these pages to determine exam type:\n\n"
    for page in selected_pages:
        image_marker = f"[IMAGE:data:image/png;base64,{page['image_base64']}]"
        input_text += f"Page {page['page_number']}:\n{image_marker}\n\n"
    
    # æ‰§è¡Œåˆ†ç±»
    result = await Runner.run(
        classifier_agent,
        input=input_text,
        max_turns=settings.classifier_max_turns
    )
    
    exam_type_output = result.final_output
    
    logger.info(f"âœ“ Classification result: {exam_type_output.exam_type}")
    logger.info(f"   Reasoning: {exam_type_output.reasoning}")
    if exam_type_output.confidence:
        logger.info(f"   Confidence: {exam_type_output.confidence:.2f}")
    
    return exam_type_output.exam_type


def get_classifier_prompt() -> str:
    """
    åˆ†ç±»å™¨æç¤ºè¯ï¼ˆä¸ import_v3 ä¸€è‡´ï¼‰
    """
    return """Analyze the provided pages of this exam and determine its type.

**Type1** (Separate Answer Booklet):
- Explicitly states "Use a SEPARATE writing booklet" or similar
- No blank lines or answer spaces under questions
- Questions are densely packed
- Example: "10(a)", "10(b)", "10(c)" are independent questions

**Type2** (Answer on Paper):
- Has blank lines with underscores (______) under questions
- Clear answer spaces between questions
- Questions have more spacing
- Example: "Question 21" is one complete question with sub-parts (a), (b), (c)

**Analysis Guidelines**:
1. Look for explicit instructions about where to write answers
2. Check for blank answer spaces or lines
3. Observe question density and spacing
4. Note the question numbering pattern

Return JSON with:
{
    "exam_type": "type1" or "type2",
    "reasoning": "Detailed explanation of classification decision",
    "confidence": 0.0-1.0 (optional)
}

**Important**: Base your decision on multiple indicators, not just one feature.
"""
```

---

### 6. Question Lister Agent (`agents/question_lister_agent.py`) ğŸ†•

```python
"""Question Lister Agent - List all questions from paper PDF"""

from typing import List
from loguru import logger
from agents import Agent, Runner, FileSearchTool

from ..config.settings import settings
from ..models.schemas import QuestionList, QuestionItem


def get_question_lister_prompt(exam_type: str) -> str:
    """
    ç”ŸæˆQuestion Listerçš„prompt
    
    Args:
        exam_type: "type1" or "type2"
    
    Returns:
        Prompt string
    """
    if exam_type == "type1":
        cutting_rule = """
ã€Type1 Rulesã€‘(Separate Answer Booklet):
- Question 10 is a **section title**, not an independent question
- 10(a), 10(b), 10(c) are **independent questions** (minimum splitting unit)
- 10(c)(i), 10(c)(ii) are **sub-parts** of 10(c), NOT separate questions
- Recognition pattern: ^\\d+\\([a-z]\\)$ indicates start of independent question

Example:
  10          â† Section title, NOT a question
  10(a)       â† Question 1: "10(a)"
  10(b)       â† Question 2: "10(b)"
  10(c)       â† Question 3: "10(c)"
    (i)       â† Sub-part of 10(c), NOT separate
    (ii)      â† Sub-part of 10(c), NOT separate
  11(a)       â† Question 4: "11(a)"
"""
    else:
        cutting_rule = """
ã€Type2 Rulesã€‘(Answer on Paper):
- "Question 21" is **one complete question** (minimum splitting unit)
- 21(a), 21(b), 21(c) are **sub-parts**, NOT separate questions
- Recognition pattern: ^Question \\d+$ indicates start of question

Example:
  Question 21    â† Question 1: "Question 21"
    (a)          â† Sub-part, NOT separate
    (b)          â† Sub-part, NOT separate
  Question 22    â† Question 2: "Question 22"
    (a)          â† Sub-part, NOT separate
"""
    
    return f"""You are a Question Lister Agent. Your task is to scan the entire paper PDF and create a **complete, accurate list** of all questions.

=== Exam Type ===
{exam_type}

=== Question Splitting Rules ===
{cutting_rule}

=== Your Task ===
1. Use the FileSearchTool to systematically scan the entire paper PDF
2. Identify ALL questions in the document
3. For each question, record:
   - question_index: Sequential number starting from 1 (1, 2, 3, ...)
   - question_label: **Exact label** as it appears in the paper (e.g., "10(a)", "Question 21")

=== Search Strategy ===
- Start from the beginning of the document
- Search for question patterns systematically
- Don't skip any sections
- Verify you've reached the end of the exam
- Double-check the count

=== Critical Rules ===
âœ… DO:
- Follow the splitting rules **strictly**
- Preserve exact question labels (including parentheses, capitalization)
- Number questions sequentially (1, 2, 3, ...)
- Include ALL questions, no matter how short

âŒ DON'T:
- Split sub-parts into separate questions
- Guess or skip questions
- Change the question labels
- Include section titles as questions (for type1)

=== Output Format ===
Return a QuestionList with:
{{
    "exam_type": "{exam_type}",
    "total_questions": <count>,
    "questions": [
        {{"question_index": 1, "question_label": "..."}},
        {{"question_index": 2, "question_label": "..."}},
        ...
    ]
}}

=== Quality Check ===
Before returning, verify:
1. total_questions == len(questions)
2. question_index are sequential (1, 2, 3, ...)
3. No duplicate question_labels
4. All question_labels follow the format rules

Begin scanning now using the FileSearchTool. Be thorough and accurate!
"""


async def list_all_questions(
    exam_type: str,
    paper_file_id: str
) -> QuestionList:
    """
    åˆ—å‡ºpaperä¸­çš„æ‰€æœ‰é¢˜ç›®
    
    Args:
        exam_type: è¯•å·ç±»å‹ ("type1" or "type2")
        paper_file_id: å·²ä¸Šä¼ çš„paper file ID
    
    Returns:
        QuestionList: åŒ…å«æ‰€æœ‰é¢˜ç›®çš„æ¸…å•
    
    Raises:
        Exception: å¦‚æœAgentæ‰§è¡Œå¤±è´¥
    """
    logger.info(f"ğŸ“‹ Listing all questions from paper...")
    logger.info(f"   Exam type: {exam_type}")
    logger.info(f"   Paper file ID: {paper_file_id}")
    
    # åˆ›å»ºFileSearchTool
    file_search = FileSearchTool(file_ids=[paper_file_id])
    
    # åˆ›å»ºAgent
    lister_agent = Agent(
        name="Question Lister",
        instructions=get_question_lister_prompt(exam_type),
        tools=[file_search],
        output_type=QuestionList,
        model=settings.openai_model
    )
    
    # æ‰§è¡Œ
    try:
        result = await Runner.run(
            lister_agent,
            input="List all questions from the paper PDF. Be systematic and thorough.",
            max_turns=settings.lister_max_turns
        )
        
        question_list = result.final_output
        
        # éªŒè¯ä¸€è‡´æ€§
        if not question_list.validate_consistency():
            logger.warning(
                f"âš ï¸  Inconsistency detected: total_questions={question_list.total_questions}, "
                f"actual count={len(question_list.questions)}"
            )
        
        logger.info(f"âœ“ Found {question_list.total_questions} questions")
        
        # æ˜¾ç¤ºå‰å‡ é“é¢˜
        preview_count = min(5, len(question_list.questions))
        for q in question_list.questions[:preview_count]:
            logger.info(f"  [{q.question_index}] {q.question_label}")
        
        if question_list.total_questions > preview_count:
            logger.info(f"  ... and {question_list.total_questions - preview_count} more")
        
        return question_list
        
    except Exception as e:
        logger.error(f"Failed to list questions: {e}")
        logger.exception(e)
        raise
```

---

### 7. File-Based Question Processor (`agents/file_based_question_processor.py`) ğŸ†•

```python
"""File-Based Question Processor - Process questions using file IDs"""

from typing import List, Optional
from loguru import logger
from agents import Agent, Runner, FileSearchTool

from ..config.settings import settings
from ..models.schemas import QuestionOutput, QuestionList
from ..tools.latex_compiler import compile_latex
from .safety_controller import safety_controller


def get_file_based_processor_prompt(
    exam_type: str,
    question_label: str
) -> str:
    """
    ç”ŸæˆåŸºäºfile_idçš„Question Processor prompt
    
    Args:
        exam_type: "type1" or "type2"
        question_label: é¢˜ç›®æ ‡ç­¾ (å¦‚ "10(a)", "Question 21")
    
    Returns:
        Prompt string
    """
    if exam_type == "type1":
        content_rule = "Keep all sub-parts (i), (ii), (iii) in the question/answer content"
    else:
        content_rule = "Keep all sub-parts (a), (b), (c) in the question/answer content"
    
    return f"""You are a Question Processor Agent. Your task is to extract and process question **{question_label}** from the PDFs.

=== Target Question ===
{question_label}

=== Available Resources ===
You have access to two PDF files via FileSearchTool:
1. **Paper PDF** - contains the question text
2. **Solution PDF** - contains the answer

=== Processing Workflow ===

**Step 1: Find the Question** ğŸ”
- Use FileSearchTool to search for "{question_label}" in the paper
- Read the **complete** question text
- Important: The question may span multiple pages
- Include ALL content until the next question starts

**Step 2: Find the Answer** ğŸ”
- Use FileSearchTool to search for the answer to "{question_label}" in the solution
- Read the **complete** answer text
- Important: The answer may span multiple pages
- Include ALL content until the next answer starts

**Step 3: Generate LaTeX** âœï¸
- Convert question text to `question_latex`
- Convert answer text to `answer_latex`
- Rules:
  - {content_rule}
  - Use proper LaTeX formatting
  - Preserve mathematical notation
  - Use \\textbf, \\textit for emphasis
  - Use \\begin{{enumerate}}, \\begin{{itemize}} for lists

**Step 4: Identify Images** ğŸ–¼ï¸
- Mark approximate locations of all images
- For each image provide:
  - page_number: which page the image appears on
  - bbox: [x1, y1, x2, y2] in PDF coordinates (origin at top-left)
  - description: brief description of the image

**Step 5: Extract Marks** ğŸ¯
- Look for marks notation like [5], [8 marks], etc.
- Extract the numeric value

**Step 6: Verify LaTeX** âœ…
- Call compile_latex(question_latex, "question") to verify
- Call compile_latex(answer_latex, "answer") to verify
- If compilation fails:
  - Analyze the error message
  - Fix the LaTeX syntax
  - Retry compilation (max {settings.max_latex_fix_attempts} attempts)

=== Output Format ===
Return a QuestionOutput with:
{{
    "question_index": <will be set externally>,
    "question_number": "{question_label}",
    "question_latex": "...",
    "answer_latex": "...",
    "question_images": [...],
    "answer_images": [...],
    "marks": <number or null>,
    "reasoning": "Brief explanation of your process"
}}

=== Important Notes ===
- **Be thorough**: Extract ALL content, don't truncate
- **Use FileSearchTool**: Don't guess, always search
- **Multi-page handling**: If content spans pages, search multiple times
- **Image bbox**: Estimate as best as you can, format [x1, y1, x2, y2]
- **LaTeX quality**: Ensure it compiles successfully

Begin processing question {question_label} now.
"""


async def process_question_from_files(
    question_index: int,
    question_label: str,
    paper_file_id: str,
    solution_file_id: str,
    exam_type: str
) -> QuestionOutput:
    """
    ä»file_idå¤„ç†å•é“é¢˜ç›®
    
    Args:
        question_index: é¢˜ç›®åºå· (1-based)
        question_label: é¢˜ç›®æ ‡ç­¾ (å¦‚ "10(a)", "Question 21")
        paper_file_id: Paper file ID
        solution_file_id: Solution file ID
        exam_type: è¯•å·ç±»å‹ ("type1" or "type2")
    
    Returns:
        QuestionOutput: å¤„ç†åçš„é¢˜ç›®æ•°æ®
    
    Raises:
        Exception: å¦‚æœå¤„ç†å¤±è´¥
    """
    logger.info(f"âš™ï¸  Processing question {question_index}: {question_label}")
    
    # Reset safety controller
    safety_controller.reset()
    
    # åˆ›å»ºFileSearchToolï¼ˆåŒæ—¶æœç´¢ä¸¤ä¸ªæ–‡ä»¶ï¼‰
    file_search = FileSearchTool(
        file_ids=[paper_file_id, solution_file_id]
    )
    
    # åˆ›å»ºAgent
    processor_agent = Agent(
        name=f"Question Processor - {question_label}",
        instructions=get_file_based_processor_prompt(exam_type, question_label),
        tools=[file_search, compile_latex],
        output_type=QuestionOutput,
        model=settings.openai_model
    )
    
    # æ‰§è¡Œ
    try:
        result = await Runner.run(
            processor_agent,
            input=f"Process question {question_label} from the PDFs",
            max_turns=settings.max_turns_per_question
        )
        
        question_data = result.final_output
        
        # è®¾ç½®question_index
        question_data.question_index = question_index
        
        # éªŒè¯question_numberåŒ¹é…
        if question_data.question_number != question_label:
            logger.warning(
                f"âš ï¸  Question label mismatch: expected '{question_label}', "
                f"got '{question_data.question_number}'"
            )
            # å¼ºåˆ¶ä¿®æ­£
            question_data.question_number = question_label
        
        logger.info(f"âœ“ Completed question {question_index}: {question_label}")
        logger.info(f"   Question LaTeX: {len(question_data.question_latex)} chars")
        logger.info(f"   Answer LaTeX: {len(question_data.answer_latex)} chars")
        logger.info(f"   Images: {len(question_data.question_images)} (Q) + {len(question_data.answer_images)} (A)")
        
        return question_data
        
    except Exception as e:
        logger.error(f"âŒ Failed to process question {question_label}: {e}")
        logger.exception(e)
        raise


async def process_all_questions_from_files(
    question_list: QuestionList,
    paper_file_id: str,
    solution_file_id: str,
    exam_type: str,
    continue_on_error: bool = True
) -> List[QuestionOutput]:
    """
    åŸºäºé¢˜ç›®æ¸…å•å¤„ç†æ‰€æœ‰é¢˜ç›®
    
    Args:
        question_list: Question Listerç”Ÿæˆçš„é¢˜ç›®æ¸…å•
        paper_file_id: Paper file ID
        solution_file_id: Solution file ID
        exam_type: è¯•å·ç±»å‹
        continue_on_error: é‡åˆ°é”™è¯¯æ˜¯å¦ç»§ç»­å¤„ç†åç»­é¢˜ç›®
    
    Returns:
        å¤„ç†æˆåŠŸçš„æ‰€æœ‰é¢˜ç›®åˆ—è¡¨
    """
    questions = []
    failed_questions = []
    
    total = question_list.total_questions
    logger.info(f"ğŸš€ Starting to process {total} questions...")
    
    for idx, question_item in enumerate(question_list.questions, 1):
        logger.info(f"\n{'='*60}")
        logger.info(f"Progress: {idx}/{total}")
        logger.info(f"{'='*60}")
        
        try:
            question_output = await process_question_from_files(
                question_index=question_item.question_index,
                question_label=question_item.question_label,
                paper_file_id=paper_file_id,
                solution_file_id=solution_file_id,
                exam_type=exam_type
            )
            questions.append(question_output)
            
        except Exception as e:
            logger.error(f"âŒ Question {question_item.question_label} failed: {e}")
            failed_questions.append(question_item.question_label)
            
            if not continue_on_error:
                logger.error("Stopping due to error (continue_on_error=False)")
                raise
            
            logger.info("Continuing to next question...")
    
    # æ€»ç»“
    logger.info(f"\n{'='*60}")
    logger.info(f"Processing Summary")
    logger.info(f"{'='*60}")
    logger.info(f"âœ“ Successful: {len(questions)}/{total}")
    if failed_questions:
        logger.warning(f"âŒ Failed: {len(failed_questions)}/{total}")
        logger.warning(f"   Failed questions: {', '.join(failed_questions)}")
    
    return questions
```

---

### 8. å·¥ä½œæµé€»è¾‘ (`workflow.py`) ğŸ†•

```python
"""Complete workflow logic for V3 File-Based processing"""

import time
from pathlib import Path
from typing import Optional
from loguru import logger

from .config.settings import settings
from .models.schemas import ProcessedExam
from .preprocessing.pdf_renderer import preprocess_for_classification
from .services.file_uploader import (
    upload_pdfs_get_file_ids,
    cleanup_files,
    FileUploadResult
)
from .agents.classifier_agent import classify_exam_type
from .agents.question_lister_agent import list_all_questions
from .agents.file_based_question_processor import process_all_questions_from_files


async def run_file_based_workflow(
    paper_pdf_path: str,
    solution_pdf_path: str,
    exam_id: Optional[str] = None,
    output_dir: Optional[str] = None
) -> ProcessedExam:
    """
    æ‰§è¡Œå®Œæ•´çš„V3 File-Based Workflow
    
    Args:
        paper_pdf_path: Paper PDFè·¯å¾„
        solution_pdf_path: Solution PDFè·¯å¾„
        exam_id: è¯•å·IDï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨æ—¶é—´æˆ³ï¼‰
        output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        ProcessedExam: å¤„ç†ç»“æœ
    
    Workflow Steps:
        1. è½»é‡çº§é¢„å¤„ç†ï¼ˆä»…å‰3é¡µç”¨äºåˆ†ç±»ï¼‰
        2. åˆ†ç±»å™¨åˆ¤æ–­è¯•å·ç±»å‹
        3. ä¸Šä¼ PDFè·å–file_id
        4. Question Listeråˆ—å‡ºæ‰€æœ‰é¢˜ç›®
        5. Question Processoré€é¢˜å¤„ç†
        6. åå¤„ç†å’Œä¿å­˜ç»“æœ
        7. ï¼ˆå¯é€‰ï¼‰æ¸…ç†ä¸Šä¼ çš„æ–‡ä»¶
    """
    start_time = time.time()
    
    # === Setup ===
    if exam_id is None:
        exam_id = f"exam_{int(time.time())}"
    
    if output_dir is None:
        output_dir = Path(settings.output_dir) / exam_id
    else:
        output_dir = Path(output_dir)
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info("="*80)
    logger.info(f"ğŸš€ V3 File-Based Workflow - Starting")
    logger.info("="*80)
    logger.info(f"Exam ID: {exam_id}")
    logger.info(f"Paper: {paper_pdf_path}")
    logger.info(f"Solution: {solution_pdf_path}")
    logger.info(f"Output: {output_dir}")
    logger.info("="*80)
    
    file_upload_result: Optional[FileUploadResult] = None
    
    try:
        # === Step 1: è½»é‡çº§é¢„å¤„ç† ===
        logger.info("\n" + "="*80)
        logger.info("Step 1: Lightweight Preprocessing (Classification Only)")
        logger.info("="*80)
        
        classification_data = await preprocess_for_classification(paper_pdf_path)
        logger.info(f"âœ“ Step 1 complete - Rendered {len(classification_data['selected_pages'])} pages")
        
        # === Step 2: åˆ†ç±»å™¨ ===
        logger.info("\n" + "="*80)
        logger.info("Step 2: Exam Type Classification")
        logger.info("="*80)
        
        exam_type = await classify_exam_type(classification_data)
        logger.info(f"âœ“ Step 2 complete - Exam type: {exam_type}")
        
        # === Step 3: ä¸Šä¼ PDFè·å–file_id ===
        logger.info("\n" + "="*80)
        logger.info("Step 3: Uploading PDFs to OpenAI")
        logger.info("="*80)
        
        file_upload_result = await upload_pdfs_get_file_ids(
            paper_pdf_path,
            solution_pdf_path
        )
        logger.info(f"âœ“ Step 3 complete")
        logger.info(f"   Paper file ID: {file_upload_result.paper_file_id}")
        logger.info(f"   Solution file ID: {file_upload_result.solution_file_id}")
        
        # === Step 4: Question Lister ===
        logger.info("\n" + "="*80)
        logger.info("Step 4: Listing All Questions")
        logger.info("="*80)
        
        question_list = await list_all_questions(
            exam_type=exam_type,
            paper_file_id=file_upload_result.paper_file_id
        )
        logger.info(f"âœ“ Step 4 complete - Found {question_list.total_questions} questions")
        
        # ä¿å­˜é¢˜ç›®æ¸…å•
        if settings.save_question_list:
            question_list_file = output_dir / "question_list.json"
            question_list_file.write_text(
                question_list.model_dump_json(indent=2),
                encoding='utf-8'
            )
            logger.info(f"   Saved question list to: {question_list_file}")
        
        # === Step 5: Question Processor ===
        logger.info("\n" + "="*80)
        logger.info("Step 5: Processing All Questions")
        logger.info("="*80)
        
        questions = await process_all_questions_from_files(
            question_list=question_list,
            paper_file_id=file_upload_result.paper_file_id,
            solution_file_id=file_upload_result.solution_file_id,
            exam_type=exam_type,
            continue_on_error=True  # ç»§ç»­å¤„ç†åç»­é¢˜ç›®
        )
        logger.info(f"âœ“ Step 5 complete - Processed {len(questions)} questions")
        
        # === Step 6: æ„å»ºç»“æœ ===
        processing_time = time.time() - start_time
        
        result = ProcessedExam(
            exam_id=exam_id,
            exam_type=exam_type,
            total_questions=len(questions),
            questions=questions,
            paper_pdf_path=paper_pdf_path,
            solution_pdf_path=solution_pdf_path,
            paper_file_id=file_upload_result.paper_file_id,
            solution_file_id=file_upload_result.solution_file_id,
            processing_time_seconds=processing_time,
            workflow_version="v3_file_based"
        )
        
        # ä¿å­˜ç»“æœ
        result_file = output_dir / f"{exam_id}_processed.json"
        result_file.write_text(
            result.model_dump_json(indent=2),
            encoding='utf-8'
        )
        logger.info(f"   Saved result to: {result_file}")
        
        # === æœ€ç»ˆæ€»ç»“ ===
        logger.info("\n" + "="*80)
        logger.info("âœ… Processing Complete!")
        logger.info("="*80)
        logger.info(f"Exam ID: {exam_id}")
        logger.info(f"Exam Type: {exam_type}")
        logger.info(f"Total Questions: {len(questions)}")
        logger.info(f"Processing Time: {processing_time:.1f}s")
        logger.info(f"Output Directory: {output_dir}")
        logger.info("="*80)
        
        return result
        
    finally:
        # === Step 7: æ¸…ç†æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰ ===
        if settings.auto_cleanup_files and file_upload_result:
            logger.info("\n" + "="*80)
            logger.info("Step 7: Cleaning Up Files")
            logger.info("="*80)
            
            await cleanup_files([
                file_upload_result.paper_file_id,
                file_upload_result.solution_file_id
            ])
            logger.info("âœ“ Step 7 complete - Files cleaned up")
```

---

### 9. ä¸»å…¥å£ (`main.py`)

```python
"""Main entry point for import_v4 - delegates to workflow"""

from typing import Optional
from .models.schemas import ProcessedExam
from .workflow import run_file_based_workflow


async def process_exam_file_based(
    paper_pdf_path: str,
    solution_pdf_path: str,
    exam_id: Optional[str] = None,
    output_dir: Optional[str] = None
) -> ProcessedExam:
    """
    V3 File-Based Workflow ä¸»å…¥å£
    
    Args:
        paper_pdf_path: Paper PDFè·¯å¾„
        solution_pdf_path: Solution PDFè·¯å¾„
        exam_id: è¯•å·IDï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨æ—¶é—´æˆ³ï¼‰
        output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        ProcessedExam: å¤„ç†ç»“æœ
    """
    return await run_file_based_workflow(
        paper_pdf_path=paper_pdf_path,
        solution_pdf_path=solution_pdf_path,
        exam_id=exam_id,
        output_dir=output_dir
    )


async def process_exam(
    paper_pdf_path: str,
    solution_pdf_path: str,
    exam_id: Optional[str] = None,
    output_dir: Optional[str] = None
) -> ProcessedExam:
    """
    å‘åå…¼å®¹çš„ä¸»å…¥å£å‡½æ•°
    
    Args:
        paper_pdf_path: Paper PDFè·¯å¾„
        solution_pdf_path: Solution PDFè·¯å¾„
        exam_id: è¯•å·IDï¼ˆå¯é€‰ï¼‰
        output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        ProcessedExam: å¤„ç†ç»“æœ
    """
    return await process_exam_file_based(
        paper_pdf_path, solution_pdf_path, exam_id, output_dir
    )
```

---

### 10. æµ‹è¯•è„šæœ¬ç¤ºä¾‹

åˆ›å»º `test_file_based.py`:

```python
"""Test script for V3 File-Based Workflow"""

import asyncio
from pathlib import Path
from loguru import logger

from import_v4.main import process_exam_file_based
from import_v4.config.settings import settings
from import_v4.utils.logger import setup_logger


async def main():
    """æµ‹è¯•ä¸»å‡½æ•°"""
    
    # è®¾ç½®æ—¥å¿—
    setup_logger()
    
    # æµ‹è¯•æ–‡ä»¶è·¯å¾„
    test_dir = Path(__file__).parent.parent / "test" / "test_input"
    paper_pdf = test_dir / "paper.pdf"
    solution_pdf = test_dir / "solution.pdf"
    
    if not paper_pdf.exists():
        logger.error(f"Paper PDF not found: {paper_pdf}")
        return
    
    if not solution_pdf.exists():
        logger.error(f"Solution PDF not found: {solution_pdf}")
        return
    
    # è¿è¡Œå¤„ç†
    try:
        result = await process_exam_file_based(
            paper_pdf_path=str(paper_pdf),
            solution_pdf_path=str(solution_pdf),
            exam_id="test_file_based_001",
            output_dir="test_output/test_file_based_001"
        )
        
        logger.info("\n" + "="*80)
        logger.info("Test Results:")
        logger.info("="*80)
        logger.info(f"Exam Type: {result.exam_type}")
        logger.info(f"Total Questions: {result.total_questions}")
        logger.info(f"Processing Time: {result.processing_time_seconds:.1f}s")
        logger.info(f"Paper File ID: {result.paper_file_id}")
        logger.info(f"Solution File ID: {result.solution_file_id}")
        
        # æ˜¾ç¤ºå‰å‡ é“é¢˜
        for i, q in enumerate(result.questions[:3], 1):
            logger.info(f"\nQuestion {i}: {q.question_number}")
            logger.info(f"  Marks: {q.marks}")
            logger.info(f"  Question LaTeX length: {len(q.question_latex)}")
            logger.info(f"  Answer LaTeX length: {len(q.answer_latex)}")
        
    except Exception as e:
        logger.error(f"Test failed: {e}")
        logger.exception(e)


if __name__ == "__main__":
    asyncio.run(main())
```

---

## å…³é”®æ¥å£å’Œæ•°æ®æµ

### æ¶æ„å±‚æ¬¡

```
main.py (å…¥å£å±‚)
    â†“ è°ƒç”¨
workflow.py (å·¥ä½œæµå±‚)
    â†“ ç¼–æ’
å„ä¸ªæ¨¡å— (æ‰§è¡Œå±‚)
```

### æ•°æ®æµå›¾

```
Input PDFs
    â†“
main.py: process_exam_file_based()
    â†“
workflow.py: run_file_based_workflow()
    â†“
[preprocess_for_classification]
    â†“
selected_pages (å€’æ•°ç¬¬2ã€4ã€6é¡µï¼Œæˆ–æœ€åNé¡µ)
    â†“
[classify_exam_type]
    â†“
exam_type
    â†“
[upload_pdfs_get_file_ids]
    â†“
FileUploadResult {paper_file_id, solution_file_id}
    â†“
[list_all_questions]
    â†“
QuestionList {exam_type, total_questions, questions[]}
    â†“
[process_all_questions_from_files] â† å¾ªç¯
    â†“
List[QuestionOutput]
    â†“
ProcessedExam
    â†“ è¿”å›
main.py
```

### å…³é”®æ¥å£æ€»ç»“

| æ¨¡å— | è¾“å…¥ | è¾“å‡º |
|------|------|------|
| `main.py::process_exam_file_based` | paper_pdf, solution_pdf, exam_id, output_dir | ProcessedExam |
| `workflow.py::run_file_based_workflow` | paper_pdf, solution_pdf, exam_id, output_dir | ProcessedExam |
| `preprocess_for_classification` | paper_pdf_path | {selected_pages, total_pages} |
| `classify_exam_type` | classification_data | exam_type |
| `upload_pdfs_get_file_ids` | paper_pdf, solution_pdf | FileUploadResult |
| `list_all_questions` | exam_type, paper_file_id | QuestionList |
| `process_question_from_files` | question_item, file_ids, exam_type | QuestionOutput |
| `process_all_questions_from_files` | question_list, file_ids | List[QuestionOutput] |

### æ¶æ„è®¾è®¡ä¼˜åŠ¿

**å…³æ³¨ç‚¹åˆ†ç¦»**ï¼š
- `main.py` - çº¯ç²¹çš„å…¥å£ç‚¹ï¼Œæä¾›ç®€æ´çš„APIæ¥å£
- `workflow.py` - ä¸“æ³¨äºå·¥ä½œæµç¼–æ’å’Œæ­¥éª¤åè°ƒ
- å„æ¨¡å— - ä¸“æ³¨äºå…·ä½“åŠŸèƒ½å®ç°

**å¯ç»´æŠ¤æ€§**ï¼š
- å·¥ä½œæµé€»è¾‘é›†ä¸­åœ¨ `workflow.py`ï¼Œä¾¿äºç†è§£å’Œä¿®æ”¹
- æ–°å¢å·¥ä½œæµæ­¥éª¤åªéœ€ä¿®æ”¹ `workflow.py`
- å…¥å£æ¥å£ä¿æŒç¨³å®šï¼Œä¸å—å·¥ä½œæµå˜åŒ–å½±å“

**å¯æµ‹è¯•æ€§**ï¼š
- å¯ä»¥ç›´æ¥æµ‹è¯• `workflow.py` çš„å·¥ä½œæµé€»è¾‘
- å¯ä»¥å•ç‹¬æµ‹è¯• `main.py` çš„æ¥å£å±‚
- å„ä¸ªæ¨¡å—å¯ä»¥ç‹¬ç«‹å•å…ƒæµ‹è¯•

**å¯æ‰©å±•æ€§**ï¼š
- æœªæ¥å¯ä»¥åœ¨ `workflow.py` ä¸­æ·»åŠ å¤šç§å·¥ä½œæµï¼ˆå¦‚ v4, v5ï¼‰
- `main.py` å¯ä»¥æ ¹æ®å‚æ•°é€‰æ‹©ä¸åŒçš„å·¥ä½œæµ
- ä¿æŒå‘åå…¼å®¹æ€§

### é¡µé¢é€‰æ‹©ç­–ç•¥è¯´æ˜

**ä¸ºä»€ä¹ˆä½¿ç”¨å€’æ•°ç¬¬ 2ã€4ã€6 é¡µè¿›è¡Œåˆ†ç±»ï¼Ÿ**

1. **ç­”é¢˜åŒºåŸŸåˆ¤æ–­æ›´å‡†ç¡®**
   - è¯•å·å‰é¢é€šå¸¸æ˜¯é¢˜ç›®è¯´æ˜å’Œå¼€å§‹éƒ¨åˆ†
   - è¯•å·åé¢æ›´å¯èƒ½åŒ…å«ç­”é¢˜åŒºåŸŸ
   - Type1 åé¢ç»§ç»­å¯†é›†é¢˜ç›®ï¼ŒType2 åé¢æœ‰æ˜æ˜¾ç©ºç™½

2. **é¿å…å°é¢å¹²æ‰°**
   - ç¬¬ä¸€é¡µé€šå¸¸æ˜¯å°é¢ã€è¯´æ˜
   - å¯èƒ½ä¸åŒ…å«å®é™…é¢˜ç›®å†…å®¹
   - å¯¹åˆ†ç±»å¸®åŠ©ä¸å¤§

3. **é‡‡æ ·å‡åŒ€**
   - å€’æ•°ç¬¬ 2ã€4ã€6 é¡µåˆ†å¸ƒè¾ƒå‡åŒ€
   - å¯ä»¥è¦†ç›–è¯•å·ä¸­åéƒ¨åˆ†çš„ä¸åŒåŒºåŸŸ
   - æé«˜åˆ†ç±»å‡†ç¡®æ€§

4. **ä¸ import_v3 ä¿æŒä¸€è‡´**
   - å·²ç»è¿‡å®é™…éªŒè¯çš„ç­–ç•¥
   - åˆ†ç±»å‡†ç¡®ç‡è¾ƒé«˜
   - ä¿æŒç³»ç»Ÿä¸€è‡´æ€§

---

## é…ç½®ç¤ºä¾‹

åˆ›å»º `.env` æ–‡ä»¶:

```bash
# OpenAIé…ç½®
EXAM_PROCESSOR_OPENAI_API_KEY=sk-your-api-key-here
EXAM_PROCESSOR_OPENAI_MODEL=gpt-4o

# Agenté…ç½®
EXAM_PROCESSOR_MAX_TURNS_PER_QUESTION=15
EXAM_PROCESSOR_MAX_LATEX_FIX_ATTEMPTS=2
EXAM_PROCESSOR_LISTER_MAX_TURNS=10
EXAM_PROCESSOR_CLASSIFIER_MAX_TURNS=5

# æ–‡ä»¶é…ç½®
EXAM_PROCESSOR_AUTO_CLEANUP_FILES=false
EXAM_PROCESSOR_SAVE_QUESTION_LIST=true

# è¾“å‡ºé…ç½®
EXAM_PROCESSOR_OUTPUT_DIR=output
```

---

## æµ‹è¯•ç­–ç•¥

### å•å…ƒæµ‹è¯•

```python
# tests/test_file_uploader.py
import pytest
from import_v4.services.file_uploader import upload_pdfs_get_file_ids

@pytest.mark.asyncio
async def test_upload_pdfs():
    result = await upload_pdfs_get_file_ids("paper.pdf", "solution.pdf")
    assert result.paper_file_id is not None
    assert result.solution_file_id is not None
```

### é›†æˆæµ‹è¯•

```python
# tests/test_integration.py
import pytest
from import_v4.main import process_exam_file_based

@pytest.mark.asyncio
async def test_full_workflow():
    result = await process_exam_file_based(
        "test_paper.pdf",
        "test_solution.pdf",
        exam_id="test_001"
    )
    assert result.total_questions > 0
    assert result.exam_type in ["type1", "type2"]
```

---

## æ€§èƒ½ç›‘æ§

åœ¨ä»£ç ä¸­æ·»åŠ æ€§èƒ½æ—¥å¿—:

```python
import time

def log_performance(step_name: str):
    def decorator(func):
        async def wrapper(*args, **kwargs):
            start = time.time()
            result = await func(*args, **kwargs)
            duration = time.time() - start
            logger.info(f"â±ï¸  {step_name}: {duration:.2f}s")
            return result
        return wrapper
    return decorator

# ä½¿ç”¨
@log_performance("Question Lister")
async def list_all_questions(...):
    ...
```

---

## ä¸‹ä¸€æ­¥

1. âœ… å®ŒæˆåŸºç¡€æ¶æ„ä»£ç 
2. âœ… å®ç°Question Lister Agent
3. âœ… å®ç°File-Based Question Processor
4. â³ å•å…ƒæµ‹è¯•
5. â³ é›†æˆæµ‹è¯•
6. â³ æ€§èƒ½ä¼˜åŒ–
7. â³ æ–‡æ¡£å®Œå–„

---

**æ–‡æ¡£ç‰ˆæœ¬**: V1.0  
**åˆ›å»ºæ—¥æœŸ**: 2024å¹´10æœˆ  
**çŠ¶æ€**: å®ç°æŒ‡å—

