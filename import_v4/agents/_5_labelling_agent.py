"""Question Labelling Agent - Label questions with topic, subtopic, type, difficulty, and mark"""

import json
import time
from typing import List, Optional, Tuple, TYPE_CHECKING
from loguru import logger
from agents import Usage

if TYPE_CHECKING:
    from . import UsageWithDuration

from ..config.settings import settings
from ..models.schemas import QuestionLabelOutput, ImageInfo
from ..clients.client_manager import ClientManager
from ..clients.base import LLMMessage, MessageContent, MessageRole, ContentType
from ....management.topic_operations import get_all_subtopics


def get_labelling_prompt(
    question_index: int,
    question_label: str,
    question_latex: str,
    answer_latex: Optional[str],
    subtopics_list: List[dict],
    existing_mark: Optional[int] = None
) -> str:
    """
    ç”Ÿæˆé¢˜ç›®æ ‡æ³¨çš„æç¤ºè¯
    
    Args:
        question_index: é¢˜ç›®ç´¢å¼•ï¼ˆé¡ºåºå·ï¼‰
        question_label: é¢˜ç›®æ ‡ç­¾
        question_latex: é¢˜ç›® LaTeX ä»£ç 
        answer_latex: ç­”æ¡ˆ LaTeX ä»£ç ï¼ˆå¯é€‰ï¼‰
        subtopics_list: å¯ç”¨çš„ subtopic åˆ—è¡¨
        existing_mark: å·²æœ‰çš„åˆ†æ•°ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        Prompt string
    """
    # æ„å»º subtopic é€‰é¡¹åˆ—è¡¨
    # ç»Ÿä¸€å­—æ®µåå¤„ç†ï¼šæ”¯æŒ topicid/topic_id å’Œ topicname/topic_name ä¸¤ç§æ ¼å¼
    subtopics_text = "\n".join([
        f"  {idx + 1}. Topic: {s.get('topic_name') or s.get('topicname', 'N/A')} (topic_id: {s.get('topicid') or s.get('topic_id', 'N/A')}) | "
        f"Subtopic: {s.get('subtopic_name') or s.get('subtopicname', 'N/A')} (subtopic_id: {s.get('subtopicid') or s.get('subtopic_id', 'N/A')})"
        for idx, s in enumerate(subtopics_list)
    ])
    
    mark_instruction = ""
    if existing_mark is not None:
        mark_instruction = f"\n- **Mark**: The question already has a mark of {existing_mark}. Verify if this is correct based on the question content. If incorrect, extract the correct mark."
    else:
        mark_instruction = "\n- **Mark**: Extract the mark from the question (look for notations like [5], [8 marks], etc.). If not found, leave as null."
    
    answer_section = ""
    if answer_latex:
        answer_section = f"""
=== Answer Content ===
{answer_latex}

**Note**: The answer content can help you understand the question better and determine its difficulty.
"""
    
    return f"""You are a Question Labelling Agent. Your task is to analyze a question and label it with accurate metadata.

=== Question Information ===
Question Index: {question_index}
Question Label: {question_label}

=== Question Content ===
{question_latex}
{answer_section}
=== Your Task ===

You need to label this question with the following metadata:

1. **Topic and Subtopic** (MOST IMPORTANT):
   - You MUST select the MOST ACCURATE subtopic from the provided list below
   - You CANNOT create new topics or subtopics - you MUST choose from the list
   - The subtopic_id is the MOST CRITICAL field - it must be accurate
   - Provide a confidence score (0.0-1.0) for your subtopic selection
   - If you are uncertain, explain why in the reasoning field

2. **Question Type** (REQUIRED):
   - You MUST choose EXACTLY ONE from: "short answer" OR "multiple choice"
   - **Multiple Choice**: Has explicit options (A, B, C, D, etc.), usually with instructions like "circle", "select", "choose"
   - **Short Answer**: Requires students to write their answer, may have blank lines, underscores, or answer spaces
   - Look at the question structure and answer format to determine the type

3. **Difficulty** (OPTIONAL):
   - Assess the difficulty based on:
     * Complexity of concepts involved
     * Number of steps required to solve
     * Level of mathematical reasoning needed
   - Common values: "Easy", "Medium", "Hard", or specific difficulty levels
   - If uncertain, you can leave it as null

4. **Mark** (OPTIONAL):{mark_instruction}

=== Available Topics and Subtopics ===

You MUST select from this list (DO NOT create new ones):

{subtopics_text}

**CRITICAL RULES**:
- You MUST select a subtopic_id from the list above
- The subtopic_id is the MOST IMPORTANT field - accuracy is critical
- If no subtopic matches perfectly, choose the CLOSEST match and explain in reasoning
- Provide confidence score for your subtopic selection

=== Output Format ===

Return ONLY valid JSON (no markdown, no code blocks):
{{
    "question_index": {question_index},
    "question_label": "{question_label}",
    "topic_id": <integer>,
    "subtopic_id": <integer>,
    "question_type": "short answer" or "multiple choice",
    "difficulty": "<string>" or null,
    "mark": <integer> or null,
    "confidence": <float between 0.0 and 1.0>,
    "reasoning": "<detailed explanation of your decisions, especially for subtopic selection>"
}}

=== Examples ===

Example 1 (Multiple Choice):
{{
    "question_index": 3,
    "question_label": "Question 3",
    "topic_id": 15,
    "subtopic_id": 42,
    "question_type": "multiple choice",
    "difficulty": "Medium",
    "mark": 2,
    "confidence": 0.95,
    "reasoning": "This is a multiple choice question about derivatives. The subtopic 'Derivatives of Trigonometric Functions' (subtopic_id: 42) is the most accurate match. The question has 4 options (A, B, C, D) and asks to select the correct answer."
}}

Example 2 (Short Answer):
{{
    "question_index": 10,
    "question_label": "10(a)",
    "topic_id": 12,
    "subtopic_id": 28,
    "question_type": "short answer",
    "difficulty": "Hard",
    "mark": 5,
    "confidence": 0.88,
    "reasoning": "This is a short answer question about quadratic equations. The subtopic 'Solving Quadratic Equations' (subtopic_id: 28) matches well. The question requires students to show their working and write the answer. The difficulty is high because it involves completing the square method."
}}

Now analyze the question and provide the labels.
"""


async def label_question_direct(
    question_index: int,
    question_label: str,
    question_latex: str,
    answer_latex: Optional[str] = None,
    question_images: Optional[List[ImageInfo]] = None,
    subject_id: int = None,
    grade_id: int = None,
    existing_mark: Optional[int] = None
) -> Tuple[QuestionLabelOutput, "UsageWithDuration"]:
    """
    æ ‡æ³¨é¢˜ç›®ï¼ˆç›´æ¥ API è°ƒç”¨ï¼‰
    
    Args:
        question_index: é¢˜ç›®ç´¢å¼•ï¼ˆé¡ºåºå·ï¼Œ1-basedï¼‰
        question_label: é¢˜ç›®æ ‡ç­¾ï¼ˆå¦‚ "10(a)", "Question 21"ï¼‰
        question_latex: é¢˜ç›® LaTeX ä»£ç 
        answer_latex: ç­”æ¡ˆ LaTeX ä»£ç ï¼ˆå¯é€‰ï¼‰
        question_images: é¢˜ç›®ä¸­çš„å›¾ç‰‡åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        subject_id: Subject IDï¼ˆå¿…éœ€ï¼Œç”¨äºè·å–å¯ç”¨çš„ subtopicsï¼‰
        grade_id: Grade IDï¼ˆå¿…éœ€ï¼Œç”¨äºè·å–å¯ç”¨çš„ subtopicsï¼‰
        existing_mark: å·²æœ‰çš„åˆ†æ•°ï¼ˆå¯é€‰ï¼Œå¦‚æœå·²æå–ï¼‰
    
    Returns:
        Tuple[QuestionLabelOutput, UsageWithDuration]: (æ ‡æ³¨è¾“å‡º, APIä½¿ç”¨ç»Ÿè®¡å«æ—¶é—´)
    """
    from . import UsageWithDuration
    
    if subject_id is None or grade_id is None:
        raise ValueError("subject_id and grade_id are required to get available subtopics")
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # è·å–å¯ç”¨çš„ subtopics
    logger.info(f"[Label] ğŸ“‹ Fetching available subtopics for subject_id={subject_id}, grade_id={grade_id}")
    subtopics = await get_all_subtopics(
        subject_id=subject_id,
        grade_id=grade_id
    )
    
    if not subtopics:
        raise ValueError(f"No subtopics found for subject_id={subject_id}, grade_id={grade_id}")
    
    logger.info(f"[Label] âœ“ Found {len(subtopics)} available subtopics")
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = ClientManager.create_agent_client()
    
    logger.info(f"[Label] ğŸ·ï¸  Labelling question {question_label}")
    
    # æ„å»º prompt
    system_prompt = get_labelling_prompt(
        question_index=question_index,
        question_label=question_label,
        question_latex=question_latex,
        answer_latex=answer_latex,
        subtopics_list=subtopics,
        existing_mark=existing_mark
    )
    
    # æ„å»ºç”¨æˆ·æ¶ˆæ¯
    user_content = [
        MessageContent(
            type=ContentType.TEXT,
            text=f"Analyze and label question {question_label}. Return JSON."
        )
    ]
    
    # å¦‚æœæœ‰å›¾ç‰‡ï¼Œæ·»åŠ å›¾ç‰‡å†…å®¹ï¼ˆå¦‚æœå›¾ç‰‡æœ‰ base64 æ•°æ®ï¼‰
    if question_images:
        for img in question_images:
            # æ£€æŸ¥æ˜¯å¦æœ‰ image_base64 å±æ€§ï¼ˆå¯èƒ½åœ¨æŸäº›æƒ…å†µä¸‹ä¸å­˜åœ¨ï¼‰
            if hasattr(img, 'image_base64') and img.image_base64:
                user_content.append(
                    MessageContent(
                        type=ContentType.IMAGE,
                        image_base64=img.image_base64
                    )
                )
    
    messages = [
        LLMMessage(role=MessageRole.SYSTEM, content=system_prompt),
        LLMMessage(role=MessageRole.USER, content=user_content)
    ]
    
    # è°ƒç”¨ APIï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
    max_retries = 2
    current_max_tokens = 3000
    
    for retry in range(max_retries):
        try:
            response = await client.aquery(
                messages=messages,
                temperature=0.0,
                max_tokens=current_max_tokens,
                response_format={"type": "json_object"}
            )
            
            # æ£€æŸ¥å“åº”å†…å®¹
            if not response.content:
                logger.error(f"[Label] Empty response content for Q{question_index}")
                logger.error(f"[Label] Response object: {response}")
                logger.error(f"[Label] finish_reason: {response.finish_reason}")
                logger.error(f"[Label] usage: {response.usage}")
                
                # å¦‚æœæ˜¯å› ä¸ºé•¿åº¦é™åˆ¶ä¸”è¿˜æœ‰é‡è¯•æœºä¼š
                if response.finish_reason == 'length' and retry < max_retries - 1:
                    current_max_tokens = int(current_max_tokens * 1.5)
                    logger.warning(f"[Label] Response truncated. Retrying with max_tokens={current_max_tokens}")
                    continue
                else:
                    raise ValueError(
                        f"API returned empty content. finish_reason={response.finish_reason}, "
                        f"tokens={response.usage.get('completion_tokens', 0) if response.usage else 0}"
                    )
            
            # è§£æå“åº”
            response_data = json.loads(response.content)
            
            # éªŒè¯ question_type
            question_type = response_data.get("question_type", "").lower()
            if question_type not in ["short answer", "multiple choice"]:
                logger.warning(f"[Label] âš ï¸  Invalid question_type: {question_type}, defaulting to 'short answer'")
                question_type = "short answer"
            
            # æ„é€ è¾“å‡ºå¯¹è±¡
            label_output = QuestionLabelOutput(
                question_index=response_data.get("question_index", question_index),
                question_label=response_data.get("question_label", question_label),
                topic_id=response_data.get("topic_id"),
                subtopic_id=response_data.get("subtopic_id"),
                question_type=question_type,
                difficulty=response_data.get("difficulty"),
                mark=response_data.get("mark", existing_mark),
                confidence=response_data.get("confidence"),
                reasoning=response_data.get("reasoning", "")
            )
            
            # æ‰‹åŠ¨æ„é€  Usage å¯¹è±¡
            usage = Usage()
            if response.usage:
                usage.requests = 1
                usage.input_tokens = response.usage.get("prompt_tokens", 0)
                usage.output_tokens = response.usage.get("completion_tokens", 0)
                usage.total_tokens = response.usage.get("total_tokens", 0)
            
            # è®¡ç®—è€—æ—¶
            duration = time.time() - start_time
            
            # è¾“å‡ºæ—¥å¿—
            logger.info(f"[Label] âœ“ Labelled question {question_index}: {question_label}")
            logger.info(f"[Label]    Topic ID: {label_output.topic_id}, Subtopic ID: {label_output.subtopic_id}")
            logger.info(f"[Label]    Type: {label_output.question_type}, Difficulty: {label_output.difficulty}")
            logger.info(f"[Label]    Mark: {label_output.mark}, Confidence: {label_output.confidence}")
            logger.info(f"[Label]    Duration: {duration:.2f}s")
            logger.info(f"[Label]    API Usage: {usage.input_tokens} input + {usage.output_tokens} output = {usage.total_tokens} tokens")
            
            # è¿”å›å¸¦æ—¶é—´çš„ usage
            usage_with_duration = UsageWithDuration(usage=usage, duration_seconds=duration)
            return label_output, usage_with_duration
            
        except json.JSONDecodeError as e:
            logger.error(f"[Label] Failed to parse JSON (attempt {retry + 1}/{max_retries}): {e}")
            logger.error(f"[Label] Response: {response.content[:500] if response.content else '(empty)'}")
            
            # å¦‚æœè¿˜æœ‰é‡è¯•æœºä¼š
            if retry < max_retries - 1:
                current_max_tokens = int(current_max_tokens * 1.5)
                logger.warning(f"[Label] Retrying with max_tokens={current_max_tokens}")
                continue
            else:
                raise
        except Exception as e:
            logger.error(f"[Label] âŒ Failed to label question {question_label}: {e}")
            raise

